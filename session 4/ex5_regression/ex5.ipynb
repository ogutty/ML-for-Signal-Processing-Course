{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <img src=\"Electronic_Brain.png\" width=\"200\" style=\"float:left\">\n",
    "<h1> Summer 2021 ML Course.</h1>\n",
    "<h2> Exercise 5: Linear and NonLinear Regression (Anscombe Quartet)<br>Tools: Numpy, Pandas, Scikit-Learn</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"desktop-computer-icon.png\" width=\"40\" style=\"float:left\">\n",
    "<h1> &nbsp; Section A: Signal Generation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anscombe_dict = {\n",
    "    'X1' : [10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0],\n",
    "    'Y1' : [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68],\n",
    "    'X2' : [10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0],\n",
    "    'Y2' : [9.14, 8.14, 8.74, 8.77, 9.26, 8.1, 6.13, 3.1, 9.13, 7.26, 4.74],\n",
    "    'X3' : [10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0],\n",
    "    'Y3' : [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73],\n",
    "    'X4' : [8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 19.0, 8.0, 8.0, 8.0],\n",
    "    'Y4' : [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.5, 5.56, 7.91, 6.89],\n",
    "}\n",
    "anscombe_df = pd.DataFrame(anscombe_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"desktop-computer-icon.png\" width=\"40\" style=\"float:left\">\n",
    "<h1> &nbsp; Section B: First and Second Order Stats Extraction</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate first & second order stats.\n",
    "Xvars = ['X' + str(i+1) for i in range(4)]\n",
    "Yvars = ['Y' + str(i+1) for i in range(4)]\n",
    "Xmeans = [anscombe_df[Xvars[i]].mean() for i in range(4)]\n",
    "Ymeans = [anscombe_df[Yvars[i]].mean() for i in range(4)]\n",
    "Xstds = [anscombe_df[Xvars[i]].std() for i in range(4)]\n",
    "Ystds = [anscombe_df[Yvars[i]].std() for i in range(4)]\n",
    "XYcorrs = [anscombe_df[Xvars[i]].corr(anscombe_df[Yvars[i]]) for i in range(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"desktop-computer-icon.png\" width=\"40\" style=\"float:left\">\n",
    "<h1> &nbsp; Section C: Linear Regression and Visualization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear regressor for each dataset.\n",
    "# Notice: reshape(-1,1) generates a row vector, as demanded by LinearRegression() below.\n",
    "X = np.linspace(2,20,200).reshape(-1,1)\n",
    "reg_arr = {}\n",
    "for i in range(4):\n",
    "    X_train = np.array(anscombe_df[Xvars[i]]).reshape(-1, 1)\n",
    "    Y_train = np.array(anscombe_df[Yvars[i]]).reshape(-1, 1).ravel()\n",
    "    reg_arr[i] = LinearRegression().fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot everything.    \n",
    "plt.rcParams['figure.figsize'] = [15, 12]\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "anscombe_df.plot.scatter(x=\"X1\", y=\"Y1\", ax=axes[0,0], title='Dataset 1', xlim=[2, 20], ylim=[2, 14]);\n",
    "anscombe_df.plot.scatter(x=\"X2\", y=\"Y2\", ax=axes[0,1], title='Dataset 2', xlim=[2, 20], ylim=[2, 14]);\n",
    "anscombe_df.plot.scatter(x=\"X3\", y=\"Y3\", ax=axes[1,0], title='Dataset 3', xlim=[2, 20], ylim=[2, 14]);\n",
    "anscombe_df.plot.scatter(x=\"X4\", y=\"Y4\", ax=axes[1,1], title='Dataset 4', xlim=[2, 20], ylim=[2, 14]);\n",
    "axes[0,0].plot(X, reg_arr[0].predict(X), color='orange');\n",
    "axes[0,0].text(3, 13, r'$\\mu=($' + \"{:.2f}\".format(Xmeans[0]) + ',' + \"{:.2f}\".format(Ymeans[0]) +')')\n",
    "axes[0,0].text(3, 12.3, r'$\\sigma=($' + \"{:.2f}\".format(Xstds[0]) + ',' + \"{:.2f}\".format(Ystds[0]) +')')\n",
    "axes[0,1].plot(X, reg_arr[1].predict(X), color='orange');\n",
    "axes[0,1].text(3, 13, r'$\\mu=($' + \"{:.2f}\".format(Xmeans[1]) + ',' + \"{:.2f}\".format(Ymeans[1]) +')')\n",
    "axes[0,1].text(3, 12.3, r'$\\sigma=($' + \"{:.2f}\".format(Xstds[1]) + ',' + \"{:.2f}\".format(Ystds[1]) +')')\n",
    "axes[1,0].plot(X, reg_arr[2].predict(X), color='orange');\n",
    "axes[1,0].text(3, 13, r'$\\mu=($' + \"{:.2f}\".format(Xmeans[2]) + ',' + \"{:.2f}\".format(Ymeans[2]) +')')\n",
    "axes[1,0].text(3, 12.3, r'$\\sigma=($' + \"{:.2f}\".format(Xstds[2]) + ',' + \"{:.2f}\".format(Ystds[2]) +')')\n",
    "axes[1,1].plot(X, reg_arr[3].predict(X), color='orange');\n",
    "axes[1,1].text(3, 13, r'$\\mu=($' + \"{:.2f}\".format(Xmeans[3]) + ',' + \"{:.2f}\".format(Ymeans[3]) +')')\n",
    "axes[1,1].text(3, 12.3, r'$\\sigma=($' + \"{:.2f}\".format(Xstds[3]) + ',' + \"{:.2f}\".format(Ystds[3]) +')')\n",
    "fig.suptitle('The regressions, first & second order stats are identical...\\nBut the datasets surely look different.', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"desktop-computer-icon.png\" width=\"40\" style=\"float:left\">\n",
    "<h1> &nbsp; Section D: Robust Linear Regression</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import HuberRegressor, TheilSenRegressor, RANSACRegressor\n",
    "estimators = [\n",
    "    ('OLS',       LinearRegression()),\n",
    "    ('Huber',     HuberRegressor(alpha=0.0, epsilon=1.35)),\n",
    "    ('Theil-Sen', TheilSenRegressor(n_subsamples=8, fit_intercept=True, random_state=43)),\n",
    "    ('RANSAC',    RANSACRegressor(min_samples=8, random_state=43))\n",
    "]\n",
    "colors = {'OLS':'red', 'Theil-Sen':'blue', 'RANSAC':'green', 'Huber':'magenta'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot everything.    \n",
    "plt.rcParams['figure.figsize'] = [15, 12]\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "anscombe_df.plot.scatter(x=\"X1\", y=\"Y1\", ax=axes[0,0], title='Dataset 1', xlim=[2, 20], ylim=[2, 14]);\n",
    "anscombe_df.plot.scatter(x=\"X2\", y=\"Y2\", ax=axes[0,1], title='Dataset 2', xlim=[2, 20], ylim=[2, 14]);\n",
    "anscombe_df.plot.scatter(x=\"X3\", y=\"Y3\", ax=axes[1,0], title='Dataset 3', xlim=[2, 20], ylim=[2, 14]);\n",
    "anscombe_df.plot.scatter(x=\"X4\", y=\"Y4\", ax=axes[1,1], title='Dataset 4', xlim=[2, 20], ylim=[2, 14]);\n",
    "\n",
    "# Fit each robust estimator to each dataset.\n",
    "X = np.linspace(2,20,200).reshape(-1,1)\n",
    "for i in range(4):\n",
    "    X_train = np.array(anscombe_df[Xvars[i]]).reshape(-1, 1)\n",
    "    Y_train = np.array(anscombe_df[Yvars[i]]).reshape(-1, 1).ravel()\n",
    "    \n",
    "    curr_axes = axes[i//2, i%2] # This happens to coincide with the subplots' arrangement.    \n",
    "    for name, estimator in estimators:\n",
    "        reg = estimator.fit(X_train, Y_train)\n",
    "        curr_axes.plot(X, reg.predict(X), color=colors[name], label=name)\n",
    "\n",
    "    curr_axes.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"desktop-computer-icon.png\" width=\"40\" style=\"float:left\">\n",
    "<h1> &nbsp; Section E: Robust Nonlinear Regression</h1>\n",
    "<h3>Exercise:</h3> Fill in the blanks in order to train a set of robust nonlinear estimators.<br>\n",
    "Hint: use the two libraries imported at the top of the cell.<br>\n",
    "Bonus: find a solution to the warning message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Fit a cubic polynomial to the data using the same set of estimators.\n",
    "# Plot everything.    \n",
    "plt.rcParams['figure.figsize'] = [15, 12]\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "anscombe_df.plot.scatter(x=\"X1\", y=\"Y1\", ax=axes[0,0], title='Dataset 1', xlim=[2, 20], ylim=[2, 14]);\n",
    "anscombe_df.plot.scatter(x=\"X2\", y=\"Y2\", ax=axes[0,1], title='Dataset 2', xlim=[2, 20], ylim=[2, 14]);\n",
    "anscombe_df.plot.scatter(x=\"X3\", y=\"Y3\", ax=axes[1,0], title='Dataset 3', xlim=[2, 20], ylim=[2, 14]);\n",
    "anscombe_df.plot.scatter(x=\"X4\", y=\"Y4\", ax=axes[1,1], title='Dataset 4', xlim=[2, 20], ylim=[2, 14]);\n",
    "\n",
    "# Fit each robust estimator to each dataset.\n",
    "X = np.linspace(2,20,200).reshape(-1,1)\n",
    "for i in range(4):\n",
    "    X_train = np.array(anscombe_df[Xvars[i]]).reshape(-1, 1)\n",
    "    Y_train = np.array(anscombe_df[Yvars[i]]).reshape(-1, 1).ravel()\n",
    "    \n",
    "    curr_axes = axes[i//2, i%2] # This coincides with the subplots' arrangement.\n",
    "    for name, estimator in estimators:\n",
    "        # ---\n",
    "        # Please fill in to define the correct model.\n",
    "        # ---\n",
    "        model.fit(X_train, Y_train)\n",
    "        curr_axes.plot(X, model.predict(X), color=colors[name], label=name)\n",
    "\n",
    "    curr_axes.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"desktop-computer-icon.png\" width=\"40\" style=\"float:left\">\n",
    "<h1> &nbsp; Section F: Test the Regressors via LeaveOneOut</h1>\n",
    "<h3>Exercise:</h3> Fill in the blanks in order to test the regression algorithms' performance via leave-one-out.<br>\n",
    "Hint: use the training and test indices provided by the LeaveOneOut split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "sum_abs_error = {}\n",
    "for name, estimator in estimators:\n",
    "    sum_abs_error[name] = {}\n",
    "    \n",
    "    # Loop over the datasets.\n",
    "    for i in range(4):\n",
    "        sum_abs_error[name][i+1] = 0\n",
    "        X = np.array(anscombe_df[Xvars[i]]).reshape(-1, 1)\n",
    "        Y = np.array(anscombe_df[Yvars[i]]).reshape(-1, 1).ravel()\n",
    "        for train_index, test_index in loo.split(X):\n",
    "            reg = estimator.fit(X[train_index], Y[train_index])\n",
    "            # ---\n",
    "            # Please calculate prediction and ground_truth to measure the absolute error.\n",
    "            # ---\n",
    "            sum_abs_error[name][i+1] += np.abs(prediction - ground_truth)\n",
    "\n",
    "ic(sum_abs_error);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
