{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### <img src=\"Electronic_Brain.png\" width=\"200\" style=\"float:left\">\n",
    "<h1> Spring 2021 ML Course.</h1>\n",
    "<h2> Exercise 8: Midterm Hackathon!<br>Feature Engineering, Linear Regression<br>Tools: Numpy, Pandas, Scikit-Learn</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competition Rules\n",
    "1. At most 10 features allowed as inputs to regressions.\n",
    "2. At least 8500 samples in the signal need to be tested against the proposed regression.\n",
    "3. The competition is split into two categories:\n",
    "     * First category: data generated with default phase.\n",
    "     * Second category: data generated with random phase via create_signal(phase=np.random.rand()*2*np.pi) - see code below.\n",
    "4. Two solutions will be considered as a \"statistical tie\" if their mean value is within 0.2 sigma of eachother.\n",
    "5. Only use functions encountered here and in past exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"desktop-computer-icon.png\" width=\"90\" style=\"float:left; margin-right: 10px;\">\n",
    "<h1> &nbsp; Section A: Signal Generation.</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_signal(noise_sigma=0.25, phase=np.pi/4, num_cycles=10, max_lookback=1000, feature_jump=29, look_forward=500):\n",
    "\n",
    "    X = np.linspace(0, num_cycles*np.pi, 1000*num_cycles) \n",
    "    N = np.random.normal(0, noise_sigma*np.abs((np.sin(X + phase))), len(X))\n",
    "    Y = np.sin(X) + N\n",
    "\n",
    "    # Create the \"signal\" dataframe, along with exponential-moving-average mean & std.\n",
    "    # We add 10 so we only have to deal with strictly positive samples.\n",
    "    sig_df = pd.DataFrame({'time':X, 'signal':Y+10})\n",
    "\n",
    "    # Put together a big bunch of Ronny-Roshbakir style features.\n",
    "    # Ronny samples the signals' log-gains at feature_jumps intervals, up to max_lookback steps back.\n",
    "    # These features do a poor job of capturing the underlying dynamics of the problem.\n",
    "    features = {'ronny':[], 'motti':[], 'batya':[]}\n",
    "    for i in range(max_lookback):\n",
    "        if i%feature_jump==0:\n",
    "            curr_feature = 'sig_gain_' + str(i+1)\n",
    "            features['ronny'].append(curr_feature)\n",
    "            # The log-gain feature is invariant to the signal's magnitude, and therefore makes sense when trading.\n",
    "            sig_df[curr_feature] = 100*(np.log(sig_df['signal']) - np.log(sig_df['signal'].shift(periods=(i+1))))\n",
    "\n",
    "    # Motti's features are better: they extract the signal's exponentially decaying mean & standard deviation.\n",
    "    window_lengths = [5, 10, 50, 100, 1000]\n",
    "    for win_len in window_lengths:\n",
    "        # Pandas' ewm() provides exponentially weighted functions (here we use mean & std()).\n",
    "        # The win_len parameter (see below) controls the \"center-of-mass\" (\"COM\", see documentation) of the moving average.\n",
    "        # Note that the moving average considers ALL previous data, but after ~3 COMs the contribution is negligible.\n",
    "        # This also means that the first ~3 COMs of data will be less reliable then the subsequent signal.\n",
    "        # HENCE... We restrict the outputs s.t. only windows with at least the maximal window length observations are avail.\n",
    "        sig_df['ewm_mean_'+str(win_len)] = sig_df['signal'].ewm(win_len, min_periods=max(window_lengths)).mean()\n",
    "        sig_df['ewm_std_'+str(win_len)] = sig_df['signal'].ewm(win_len, min_periods=max(window_lengths)).std()\n",
    "        features['motti'].append('ewm_mean_'+str(win_len))\n",
    "        features['motti'].append('ewm_std_'+str(win_len))\n",
    "\n",
    "    # Feel free to place additional super-smart Batya-Bingo features here:\n",
    "    # sig_df['bla_bla_666'] = something_really_smart\n",
    "    # features['batya'].append('bla_bla_666')\n",
    "    \n",
    "    # The 20-day exponential moving averages and STDs are used (only) in the graphs below.\n",
    "    sig_df['ewm_mean_20'] = sig_df['signal'].ewm(20).mean()\n",
    "    sig_df['ewm_std_20'] = sig_df['signal'].ewm(20).std()\n",
    "       \n",
    "    # Extract a few \"future features\" (will be used for constructing the target).\n",
    "    # We're interested in looking (no more than) look_forward steps into the future.\n",
    "    # NOTICE: again, we restrict the outputs s.t. only windows with at least look_forward observations are avail.\n",
    "    indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=look_forward)\n",
    "    sig_df['future_mean'] = sig_df['signal'].rolling(window=indexer, min_periods=look_forward).mean()\n",
    "    sig_df['future_std'] = sig_df['signal'].rolling(window=indexer, min_periods=look_forward).std()\n",
    "    \n",
    "    # Option 1: assume access to the \"clean\" signal when calculating the future_gain (in practice of course we wouldn't).\n",
    "    sig_df['future_gain'] = 100*(np.log(sig_df['future_mean']) - np.log(10 + np.sin(X)))\n",
    "    # Option 2: in a slightly more realistic scenario we would be doing something like this:\n",
    "    # sig_df['future_gain'] = 100*(np.log(sig_df['future_mean']) - np.log(sig_df['ewm_mean']))\n",
    "\n",
    "    # Our target divides the future gain by the future standard deviation + 1 (we don't like volatility!).\n",
    "    sig_df['target'] = sig_df['future_gain'].divide(sig_df['future_std'] + 1)\n",
    "\n",
    "    return(sig_df, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show off today's signal family.\n",
    "plt.rcParams['figure.figsize'] = [15, 8]\n",
    "sig1_df, _ = create_signal(phase=0)           # The \"_\" by convention means we do not plan to use a returned variable.\n",
    "sig2_df, _ = create_signal(phase=np.pi/4)\n",
    "sig3_df, _ = create_signal(phase=np.pi/2)\n",
    "fig, axes = plt.subplots()\n",
    "plt.scatter(sig1_df['time'], sig1_df['signal'], color='b', s=1.5, label='phase=0');\n",
    "plt.scatter(sig2_df['time'], sig2_df['signal']+2, color='r', s=1.5, label='phase=pi/4');\n",
    "plt.scatter(sig3_df['time'], sig3_df['signal']+4, color='g', s=1.5, label='phase=pi/2');\n",
    "plt.title(\"Three sample signals in our family\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_df, features = create_signal()\n",
    "fig, ax = plt.subplots()\n",
    "plt.title(\"Means, STDs and target values for a sample signal\")\n",
    "plt.scatter(sig_df['time'], sig_df['signal'], color='b', s=1.5)\n",
    "ax.fill_between(sig_df['time'], sig_df['ewm_mean_20']-0.5*sig_df['ewm_std_20'], sig_df['ewm_mean_20']+0.5*sig_df['ewm_std_20'], color='r', alpha=0.25)\n",
    "ax.fill_between(sig_df['time'], sig_df['ewm_mean_20']-1.5*sig_df['ewm_std_20'], sig_df['ewm_mean_20']+1.5*sig_df['ewm_std_20'], color='g', alpha=0.25)\n",
    "ax.plot(sig_df['time'], sig_df['future_mean'], color='m', label='future_mean')\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(sig_df['time'], sig_df['target'], color='brown', label='target')\n",
    "ax2.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a dataset using only Ronny's features, clean by dropping all rows with missing data.\n",
    "ronny_clean_df = sig_df[features['ronny'] + ['target']].dropna()\n",
    "X_ronny_df = ronny_clean_df[features['ronny']]\n",
    "y_ronny = ronny_clean_df['target']\n",
    "\n",
    "motti_clean_df = sig_df[features['motti'] + ['target']].dropna()\n",
    "X_motti_df = motti_clean_df[features['motti']]\n",
    "y_motti = motti_clean_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear regressor on the continuous target.\n",
    "# NOTICE: when we predict we keep the resulting index to avoid confusion later!\n",
    "ronny_reg = LinearRegression()\n",
    "ronny_reg.fit(X_ronny_df, y_ronny)\n",
    "ic(ronny_reg.n_features_in_)\n",
    "\n",
    "# Use Motti's features to estimate the target directly.\n",
    "motti_reg = LinearRegression()\n",
    "motti_reg.fit(X_motti_df, y_motti)\n",
    "ic(motti_reg.n_features_in_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Roni.png\" width=\"50\" style=\"float:left\">\n",
    " &nbsp; Watch Ronni approach the problem his way. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_loss=[]\n",
    "for _ in range(100):\n",
    "    \n",
    "    # NOTICE: activate the first line (constant phase) for Category A of the competition.\n",
    "    # NOTICE: activate the second line (random phase) for Category B of the competition.\n",
    "    ronny_sig_df, features = create_signal()  # Category A\n",
    "    # ronny_sig_df, features = create_signal(phase=np.random.rand()*2*np.pi)  # Category B\n",
    "\n",
    "    # Extract the same set of features + target.\n",
    "    ronny_clean_df = ronny_sig_df[features['ronny'] + ['target']].dropna()\n",
    "    ronny_X_df = ronny_clean_df[features['ronny']]\n",
    "    ronny_y = ronny_clean_df['target']\n",
    "\n",
    "    # Notice we're using ronny_reg, the previously constructed regressor.\n",
    "    ronny_pred = pd.Series(ronny_reg.predict(ronny_X_df), index=ronny_X_df.index)\n",
    "    # Add the l1 loss *per sample*. This way we can compare regressors with different support sets.\n",
    "    ronny_l1_loss = np.sum(np.abs(ronny_y - ronny_pred)) / len(ronny_y)\n",
    "    l1_loss.append(ronny_l1_loss)\n",
    "\n",
    "# Show the (last in loop) output.\n",
    "fig, ax = plt.subplots()\n",
    "ax.set(xlabel='Day Seq. Number', title='Num. features: ' + str(len(features['ronny'])) + \\\n",
    "       \".  Samples tested: \" + str(len(ronny_y)) + \".  Mean L1 loss per sample: \" + \\\n",
    "       str(np.round(np.mean(l1_loss), 3)) + \" +/- \" + str(np.round(np.std(l1_loss), 3)))\n",
    "ax.plot(ronny_pred, label='Ronny prediction', color='green')\n",
    "ax.plot(ronny_y, label='target', color='brown')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Motti.png\" width=\"50\" style=\"float:left\">\n",
    " &nbsp; Now we watch Motti approach the problem...<br>\n",
    "&nbsp; Change the dataset's distribution by adding a random phase (see commented out line).<br>\n",
    "&nbsp; Before running the simulation, guess what you think should happen..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the whole procedure using Motti's features, iterating with a different random phase.\n",
    "l1_loss=[]\n",
    "for _ in range(100):\n",
    "    motti_sig_df, features = create_signal()\n",
    "    # motti_sig_df, features = create_signal(phase=np.random.rand()*2*np.pi)\n",
    "\n",
    "    # Extract the same set of features + target.\n",
    "    motti_clean_df = motti_sig_df[features['motti'] + ['target']].dropna()\n",
    "    motti_X_df = motti_clean_df[features['motti']]\n",
    "    motti_y = motti_clean_df['target']\n",
    "\n",
    "    # Notice we're using motti_reg, the previously constructed regressor.\n",
    "    motti_pred = pd.Series(motti_reg.predict(motti_X_df), index=motti_X_df.index)\n",
    "    motti_l1_loss = np.sum(np.abs(motti_y - motti_pred)) / len(motti_y)\n",
    "    l1_loss.append(motti_l1_loss)\n",
    "\n",
    "# Show the (last in loop) output.\n",
    "fig, ax = plt.subplots()\n",
    "ax.set(xlabel='Day Seq. Number', title='Num. features: ' + str(motti_reg.n_features_in_) + \\\n",
    "       \".  Samples tested: \" + str(len(motti_y)) + \".  Mean L1 loss per sample: \" + \\\n",
    "       str(np.round(np.mean(l1_loss), 3)) + \" +/- \" + str(np.round(np.std(l1_loss), 3)))\n",
    "ax.plot(motti_pred, label='Motti prediction', color='green')\n",
    "ax.plot(motti_y, label='target', color='brown')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
