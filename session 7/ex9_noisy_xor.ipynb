{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Electronic_Brain.png\" width=\"200\" style=\"float:left\">\n",
    "<h1> Summer 2021 ML Course.</h1>\n",
    "<h2> Exercise 9: Noisy Features<br>Tools: Numpy, Pandas, Scikit-Learn</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from random import gauss, randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a few defaults.\n",
    "np.random.seed(66)\n",
    "pd.set_option('display.precision', 3)\n",
    "plt.rcParams['figure.figsize'] = [15, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"desktop-computer-icon.png\" width=\"40\" style=\"float:left\">\n",
    "<h1> &nbsp; Section A: Signal Generation.</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate AAPL's \"bipolar\" behavior, alternating between price levels of 10 and 30 USD with noise, for 20 days at a time.\n",
    "Our target function today is a \"long straddle\".\n",
    "Basically, the long straddle makes money on big price swings (up *OR* down),\n",
    "and loses when prices are \"calm\" (within a relatively flat range).\n",
    "\"\"\"\n",
    "def generate_prices(sigma=5, regime_len=20, num_regimes=20, num_noise_features=10):\n",
    "    prices_df = pd.DataFrame()\n",
    "    appl_prices = np.array([])\n",
    "    for i in range(num_regimes):\n",
    "        curr_prices = 20 + 10*(-1)**i + np.random.normal(0, sigma, regime_len)\n",
    "        appl_prices = np.append(appl_prices, curr_prices)\n",
    "\n",
    "    prices_df['AAPL'] = appl_prices\n",
    "    series_len = len(prices_df['AAPL'])\n",
    "\n",
    "    # Generate backward & forward 10-day gains (*NOT* rolling means).\n",
    "    prices_df['AAPL_back_10_day_gain'] = prices_df['AAPL'] - prices_df['AAPL'].shift(10)\n",
    "    prices_df['AAPL_back_30_day_gain'] = prices_df['AAPL'] - prices_df['AAPL'].shift(30)\n",
    "    features = ['AAPL_back_10_day_gain', 'AAPL_back_30_day_gain']\n",
    "\n",
    "    # Add noise features. These can and will hurt performance.\n",
    "    for i in range(num_noise_features):\n",
    "        prices_df['noise_' + str(i)] = np.random.normal(0, 20, series_len)\n",
    "        features.append('noise_'+str(i))\n",
    "    \n",
    "    # Create the \"long straddle\" (continuous) target function.\n",
    "    prices_df['target'] = abs(prices_df['AAPL'] - prices_df['AAPL'].shift(-10)) - 20\n",
    "\n",
    "    # Return a \"clean\" version of the prices dataframe, with no missing values.\n",
    "    prices_df.dropna(inplace=True)\n",
    "    return(features, prices_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_noise_features = 15\n",
    "features, Apple_df = generate_prices(num_noise_features=num_noise_features)\n",
    "\n",
    "# A little reminder of the original time series dataset.\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(Apple_df['AAPL'], label='AAPL')\n",
    "ax.set(xlabel='Day Seq. Number', title='AAPL Prices');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Perform 25-fold cross validation to estimate the regressor's accuracy.\n",
    "Check the scikit-learn documentation to check how score() scores the regression algorithms.\n",
    "\"\"\"\n",
    "def eval_regression(reg_func, features, target_field, X_df):\n",
    "    losses = np.zeros(25)\n",
    "    for i in range(25):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_df[features], X_df[target_field], test_size=0.1)\n",
    "        reg_func.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate the regression results on the held-out test set.\n",
    "        # Option 1: evaluate the regression results using the regressors' built-in score() function (see previous exercise).\n",
    "        # Option 2: evaluate the test-set loss using a user-defined loss function.\n",
    "        # In this case, we use the mean absolute error (== L1 loss) per sample.\n",
    "        losses[i] = np.linalg.norm(y_test - reg_func.predict(X_test), ord=1) / len(y_test)\n",
    "\n",
    "    return([np.mean(losses), np.std(losses)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"desktop-computer-icon.png\" width=\"40\" style=\"float:left\">\n",
    "<h1> &nbsp; Section B: Regression Training and Visualization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_order = 2\n",
    "num_neighbs = 25\n",
    "curr_features = ['AAPL_back_10_day_gain', 'AAPL_back_30_day_gain']\n",
    "loss_means = {\n",
    "    'ols_poly' : [],\n",
    "    'knn'      : [],\n",
    "    'dec_tree' : [],\n",
    "    'boosted_tree' : [],\n",
    "}\n",
    "loss_stds = {\n",
    "    'ols_poly' : [],\n",
    "    'knn'      : [],\n",
    "    'dec_tree' : [],\n",
    "    'boosted_tree' : [],\n",
    "}\n",
    "\n",
    "# Use the tqdm progress bar.\n",
    "# It's no coincidence the word sounds familiar, the guy who wrote this library is Israeli!\n",
    "for i in tqdm(range(num_noise_features + 1)):\n",
    "    if i>0:\n",
    "        curr_features.append('noise_' + str(i-1))\n",
    "    X = Apple_df[curr_features]\n",
    "    y = Apple_df['target']\n",
    "\n",
    "    # Train a second order polynomial linear regressor on the continuous target.\n",
    "    regressor = LinearRegression()\n",
    "    ols_poly_model = make_pipeline(PolynomialFeatures(poly_order), regressor)\n",
    "    ols_poly_model.fit(X,y)\n",
    "    ols_poly_mean, ols_poly_std = eval_regression(ols_poly_model, curr_features, 'target', Apple_df)\n",
    "    loss_means['ols_poly'].append(ols_poly_mean)\n",
    "    loss_stds['ols_poly'].append(ols_poly_std)\n",
    "\n",
    "    # Add a K-nearest-neighbor regressor.\n",
    "    knn_reg = KNeighborsRegressor(n_neighbors=num_neighbs)\n",
    "    knn_reg.fit(X, y)\n",
    "    knn_mean, knn_std = eval_regression(knn_reg, curr_features, 'target', Apple_df)\n",
    "    loss_means['knn'].append(knn_mean)\n",
    "    loss_stds['knn'].append(knn_std)\n",
    "    \n",
    "    # A Decision tree.\n",
    "    dec_tree_reg = DecisionTreeRegressor()\n",
    "    dec_tree_reg.fit(X, y)\n",
    "    dec_tree_mean, dec_tree_std = eval_regression(dec_tree_reg, curr_features, 'target', Apple_df)\n",
    "    loss_means['dec_tree'].append(dec_tree_mean)\n",
    "    loss_stds['dec_tree'].append(dec_tree_std)\n",
    "    \n",
    "    # And a boosted decision tree.\n",
    "    boosted_tree_reg = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "    boosted_tree_reg.fit(X, y)\n",
    "    btree_mean, btree_std = eval_regression(boosted_tree_reg, curr_features, 'target', Apple_df)\n",
    "    loss_means['boosted_tree'].append(btree_mean)\n",
    "    loss_stds['boosted_tree'].append(btree_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the various regression algorithms' performance as a function of the number of noisy dimensions.\n",
    "plt.rcParams['figure.figsize'] = [15, 8]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(np.arange(num_noise_features+1), loss_means['knn'], label = \"KNN (25 neighbors)\", color='g', marker=\"o\")\n",
    "plt.fill_between(np.arange(num_noise_features+1),\n",
    "                 np.array(loss_means['knn']) - 0.5*np.array(loss_stds['knn']),\n",
    "                 np.array(loss_means['knn']) + 0.5*np.array(loss_stds['knn']),\n",
    "                 alpha=0.2, color='g')\n",
    "\n",
    "ax.plot(np.arange(num_noise_features+1), loss_means['ols_poly'], label = \"2nd-deg-poly LS\", color='r', marker=\"o\")\n",
    "plt.fill_between(np.arange(num_noise_features+1),\n",
    "                 np.array(loss_means['ols_poly']) - 0.5*np.array(loss_stds['ols_poly']),\n",
    "                 np.array(loss_means['ols_poly']) + 0.5*np.array(loss_stds['ols_poly']),\n",
    "                 alpha=0.2, color='r')\n",
    "\n",
    "ax.plot(np.arange(num_noise_features+1), loss_means['dec_tree'], label = \"decision tree\", color='b', marker=\"o\")\n",
    "plt.fill_between(np.arange(num_noise_features+1),\n",
    "                 np.array(loss_means['dec_tree']) - 0.5*np.array(loss_stds['dec_tree']),\n",
    "                 np.array(loss_means['dec_tree']) + 0.5*np.array(loss_stds['dec_tree']),\n",
    "                 alpha=0.2, color='b')\n",
    "\n",
    "ax.plot(np.arange(num_noise_features+1), loss_means['boosted_tree'], label = \"AdaBoost tree\", color='m', marker=\"o\")\n",
    "plt.fill_between(np.arange(num_noise_features+1),\n",
    "                 np.array(loss_means['boosted_tree']) - 0.5*np.array(loss_stds['boosted_tree']),\n",
    "                 np.array(loss_means['boosted_tree']) + 0.5*np.array(loss_stds['boosted_tree']),\n",
    "                 alpha=0.2, color='m')\n",
    "\n",
    "plt.xlabel(\"Number of noise features\")\n",
    "plt.ylabel(\"Cross-Validation L1-loss (Mean +/- 0.5*STD)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Electronic_Brain.png\" width=\"140\" style=\"float:left; margin-right: 1px;\">\n",
    "<h1>Section C: Reflection (and a few tweaks).</h1><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please (intuitively) explain the different regressors' plotted behaviors.<br>\n",
    "* Which regressors are more robust to noise? Why..?\n",
    "* What is the expected L1 loss of a \"flat\" regressor, which outputs some constant number?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please change the loss function to the L2 loss function and repeat the experiment.<br>\n",
    "Which loss function(s) are the regressors attempting to minimize?<br>\n",
    "Is there a way to train the regressors to minimize an arbitrary loss?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
