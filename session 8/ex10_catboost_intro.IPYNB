{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### <img src=\"Electronic_Brain.png\" width=\"200\" style=\"float:left\">\n",
    "<h1> Spring 2021 ML Course.</h1>\n",
    "<h2> Exercise 10: Gradient Boosting Regression<br>Tools: CatBoost</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linalg as LA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import Pool, CatBoost, CatBoostRegressor\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"desktop-computer-icon.png\" width=\"90\" style=\"float:left; margin-right: 10px;\">\n",
    "<h1> &nbsp; Section A: Signal Generation.</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_signal(noise_sigma=0.25, phase=np.pi/4, num_cycles=10, max_lookback=1000, feature_jump=29, look_forward=500):\n",
    "\n",
    "    X = np.linspace(0, num_cycles*np.pi, 1000*num_cycles) \n",
    "    N = np.random.normal(0, noise_sigma*np.abs((np.sin(X + phase))), len(X))\n",
    "    Y = np.sin(X) + N\n",
    "\n",
    "    # Create the \"signal\" dataframe, along with exponential-moving-average mean & std.\n",
    "    # We add 10 so we only have to deal with strictly positive samples.\n",
    "    sig_df = pd.DataFrame({'time':X, 'signal':Y+10, 'true_signal':np.sin(X)+10})\n",
    "\n",
    "    # Put together a big bunch of Ronny-Roshbakir style features.\n",
    "    # Ronny samples the signals' log-gains at feature_jumps intervals, up to max_lookback steps back.\n",
    "    # These features do a poor job of capturing the underlying dynamics of the problem.\n",
    "    features = {'ronny':[], 'motti':[]}\n",
    "    for i in range(max_lookback):\n",
    "        if i%feature_jump==0:\n",
    "            curr_feature = 'sig_gain_' + str(i+1)\n",
    "            features['ronny'].append(curr_feature)\n",
    "            # The log-gain feature is invariant to the signal's magnitude, and therefore makes sense when trading.\n",
    "            sig_df[curr_feature] = 100*(np.log(sig_df['signal']) - np.log(sig_df['signal'].shift(periods=(i+1))))\n",
    "\n",
    "    # Motti's features are better: they extract the signal's exponentially decaying mean & standard deviation.\n",
    "    # window_lengths = [200, 400, 600, 800, 1000] # Try an evenly spaced grid (best results so far for Category B).\n",
    "    # window_lengths = [50, 100, 200, 500, 1000]  # Or an almost-geometrically spaced grid (almost similar results on Category B).\n",
    "    window_lengths = [25, 80, 200, 500, 1000]  # Or an almost-geometrically spaced grid (almost similar results on Category B).\n",
    "    for win_len in window_lengths:\n",
    "        # Pandas' ewm() provides exponentially weighted functions (here we use mean & std()).\n",
    "        # The win_len parameter (see below) controls the \"center-of-mass\" (\"COM\", see documentation) of the moving average.\n",
    "        # Note that the moving average considers ALL previous data, but after ~3 COMs the contribution is negligible.\n",
    "        # This also means that the first ~3 COMs of data will be less reliable then the subsequent signal.\n",
    "        # HENCE... We restrict the outputs s.t. only windows with at least the maximal window length observations are avail.\n",
    "        sig_df['ewm_mean_'+str(win_len)] = sig_df['signal'].ewm(win_len, min_periods=max(window_lengths)).mean()\n",
    "        sig_df['ewm_std_'+str(win_len)] = sig_df['signal'].ewm(win_len, min_periods=max(window_lengths)).std()\n",
    "        features['motti'].append('ewm_mean_'+str(win_len))\n",
    "        features['motti'].append('ewm_std_'+str(win_len))\n",
    "\n",
    "    # The 20-day exponential moving averages and STDs are used (only) in the graphs below.\n",
    "    sig_df['ewm_mean_20'] = sig_df['signal'].ewm(20).mean()\n",
    "    sig_df['ewm_std_20'] = sig_df['signal'].ewm(20).std()\n",
    "       \n",
    "    # Extract a few \"future features\" (will be used for constructing the target).\n",
    "    # We're interested in looking (no more than) look_forward steps into the future.\n",
    "    # NOTICE: again, we restrict the outputs s.t. only windows with at least look_forward observations are avail.\n",
    "    indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=look_forward)\n",
    "    sig_df['future_mean'] = sig_df['signal'].rolling(window=indexer, min_periods=look_forward).mean()\n",
    "    sig_df['future_std'] = sig_df['signal'].rolling(window=indexer, min_periods=look_forward).std()\n",
    "    \n",
    "    # Option 1: assume access to the \"true\" signal when calculating the future_gain (in practice of course we wouldn't).\n",
    "    sig_df['future_gain'] = 100*(np.log(sig_df['future_mean']) - np.log(sig_df['true_signal']))\n",
    "    # Option 2: in a slightly more realistic scenario we would be doing something like this:\n",
    "    # sig_df['future_gain'] = 100*(np.log(sig_df['future_mean']) - np.log(sig_df['ewm_mean']))\n",
    "\n",
    "    # Our target divides the future gain by the future standard deviation + 1 (we don't like volatility!).\n",
    "    sig_df['target'] = sig_df['future_gain'].divide(sig_df['future_std'] + 1)\n",
    "\n",
    "    return(sig_df, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show off today's signal family.\n",
    "plt.rcParams['figure.figsize'] = [15, 8]\n",
    "sig_df, _ = create_signal(phase=np.pi/4)\n",
    "fig, axes = plt.subplots()\n",
    "plt.scatter(sig_df['time'], sig_df['signal'], color='b', s=1.5, label='phase=pi/4');\n",
    "plt.title(\"Today's signal: sine waves with constant phase random noise\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_df, features = create_signal()\n",
    "fig, ax = plt.subplots()\n",
    "plt.title(\"Means, STDs and target values for a sample signal\")\n",
    "plt.scatter(sig_df['time'], sig_df['signal'], color='b', s=1.5)\n",
    "ax.fill_between(sig_df['time'], sig_df['true_signal']-0.5*sig_df['ewm_std_20'], sig_df['true_signal']+0.5*sig_df['ewm_std_20'], color='r', alpha=0.25)\n",
    "ax.fill_between(sig_df['time'], sig_df['true_signal']-1.5*sig_df['ewm_std_20'], sig_df['true_signal']+1.5*sig_df['ewm_std_20'], color='g', alpha=0.25)\n",
    "ax.plot(sig_df['time'], sig_df['future_mean'], color='m', label='future_mean')\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(sig_df['time'], sig_df['target'], color='brown', label='target')\n",
    "ax2.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a dataset using only Ronny's features, clean by dropping all rows with missing data.\n",
    "ronny_clean_df = sig_df[features['ronny'] + ['target']].dropna()\n",
    "X_ronny_df = ronny_clean_df[features['ronny']]\n",
    "y_ronny = ronny_clean_df['target']\n",
    "\n",
    "motti_clean_df = sig_df[features['motti'] + ['target']].dropna()\n",
    "X_motti_df = motti_clean_df[features['motti']]\n",
    "y_motti = motti_clean_df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"desktop-computer-icon.png\" width=\"90\" style=\"float:left; margin-right: 10px;\">\n",
    "<h1> &nbsp; Section B: Regression Training.</h1>\n",
    "Unfortunately, my Windows machine (and probably all others) are not able to display the learning curve (below).<br>\n",
    "10 extra points to anyone who manages to display this chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the \"learning curve\" / \"training curve\" for Ronny.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ronny_df, y_ronny, test_size=0.25, random_state=42)\n",
    "cat_regress = CatBoostRegressor(iterations=1000)\n",
    "cat_regress.fit(X_train, y_train, plot=True, eval_set=(X_test, y_test), silent=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a few regressors on the continuous target.\n",
    "regressors = {}\n",
    "\n",
    "# Train an L1 & L2 regressors on Ronny's features.\n",
    "regressors['ronny_L1'] = CatBoostRegressor(loss_function='Lq:q=1')\n",
    "regressors['ronny_L1'].fit(X_ronny_df, y_ronny, silent=True)\n",
    "regressors['ronny_L2'] = CatBoostRegressor(loss_function='RMSE')\n",
    "regressors['ronny_L2'].fit(X_ronny_df, y_ronny, silent=True)\n",
    "\n",
    "# Repeat for Motti's features.\n",
    "regressors['motti_L1'] = CatBoostRegressor(loss_function='Lq:q=1')\n",
    "regressors['motti_L1'].fit(X_motti_df, y_motti, silent=True)\n",
    "regressors['motti_L2'] = CatBoostRegressor(loss_function='RMSE')\n",
    "regressors['motti_L2'].fit(X_motti_df, y_motti, silent=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"desktop-computer-icon.png\" width=\"90\" style=\"float:left; margin-right: 10px;\">\n",
    "Exercise: look up other (regression) loss functions available in CatBoost:<br>\n",
    "https://catboost.ai/en/docs/concepts/loss-functions-regression\n",
    "\n",
    "Please repeat the experiment for the L_8 loss function and explain the results.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions={}\n",
    "prediction_errors={}\n",
    "\n",
    "l1_losses={\n",
    "    'ronny_L2' : [],   'ronny_L1'   : [],\n",
    "    'motti_L2' : [],   'motti_L1'   : [],\n",
    "}\n",
    "l2_losses={\n",
    "    'ronny_L2' : [],   'ronny_L1'   : [],\n",
    "    'motti_L2' : [],   'motti_L1'   : [],\n",
    "}\n",
    "\n",
    "for _ in range(100):\n",
    "    for regressor in l1_losses.keys():\n",
    "        # NOTICE: activate the first line (constant phase) for Category A of the competition.\n",
    "        # NOTICE: activate the second line (random phase) for Category B of the competition.\n",
    "        sig_df, features = create_signal()\n",
    "        # sig_df, features = create_signal(phase=np.random.rand()*2*np.pi)\n",
    "\n",
    "        if 'ronny' in regressor:\n",
    "            # Extract Ronny's features + target.\n",
    "            clean_df = sig_df[features['ronny'] + ['target']].dropna()\n",
    "            X_df = clean_df[features['ronny']]\n",
    "        elif 'motti' in regressor:\n",
    "            # Extract Motti's features + target.\n",
    "            clean_df = sig_df[features['motti'] + ['target']].dropna()\n",
    "            X_df = clean_df[features['motti']]\n",
    "        \n",
    "        y = clean_df['target']\n",
    "        pred = pd.Series(regressors[regressor].predict(X_df), index=X_df.index)\n",
    "        \n",
    "        # Add the L1 & L2 losses *per sample*. This way we can compare regressors with different support sets.\n",
    "        l1_loss = LA.norm(y-pred, ord=1) / len(y)\n",
    "        l2_loss = LA.norm(y-pred, ord=2) / len(y)\n",
    "        l1_losses[regressor].append(l1_loss)\n",
    "        l2_losses[regressor].append(l2_loss)\n",
    "        \n",
    "        # Add a single predicted series per regressor (for the graphs below).\n",
    "        if regressor not in predictions:\n",
    "            predictions[regressor] = pred\n",
    "            prediction_errors[regressor] = pred - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 15]\n",
    "fig = plt.figure()\n",
    "gs = fig.add_gridspec(4, hspace=0.3)\n",
    "axs = gs.subplots(sharex=True, sharey=True)\n",
    "for i, regressor in enumerate(sorted(l1_losses.keys(), reverse=True)):\n",
    "    axs[i].plot(predictions[regressor], label=regressor+' prediction', color='green')\n",
    "    axs[i].plot(y, label='target', color='brown')\n",
    "    axs[i].set(xlabel='Day Seq. Number', title=regressor + \" regressor. Mean L1 loss per sample: \" + \\\n",
    "        str(np.round(np.mean(l1_losses[regressor]), 5)) + \" +/- \" + str(np.round(np.std(l1_losses[regressor]), 5)) + \\\n",
    "        \". Mean L2 loss per sample: \" + str(np.round(np.mean(l2_losses[regressor]), 5)) + \" +/- \" + str(np.round(np.std(l2_losses[regressor]), 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 15]\n",
    "fig = plt.figure()\n",
    "gs = fig.add_gridspec(4, hspace=0.3)\n",
    "axs = gs.subplots(sharex=True)\n",
    "for i, regressor in enumerate(sorted(l1_losses.keys(), reverse=True)):\n",
    "    axs[i].plot(prediction_errors[regressor])\n",
    "    axs[i].set(title = regressor + \" regressor\" + \\\n",
    "        \". Mean L1 loss per sample: \" + str(np.round(np.mean(l1_losses[regressor]), 5)) + \" +/- \" + str(np.round(np.std(l1_losses[regressor]), 5)) + \\\n",
    "        \". Mean L2 loss per sample: \" + str(np.round(np.mean(l2_losses[regressor]), 5)) + \" +/- \" + str(np.round(np.std(l2_losses[regressor]), 5)))\n",
    "axs[3].set(xlabel='Day Seq. Number');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"desktop-computer-icon.png\" width=\"90\" style=\"float:left; margin-right: 10px;\">\n",
    "<h1> &nbsp; Section C: Variance Estimation.</h1>\n",
    "The standard model optimized with the RMSE loss can only predict mean(x₁,x₂).\n",
    "What if we want to estimate the variance of y, i.e., data uncertainty?<br>\n",
    "To estimate data uncertainty, we need to use probabilistic regression models that predict both mean and variance.<br>\n",
    "\n",
    "For this purpose, we use the *RMSEWithUncertainty* loss function in CatBoost.<br>\n",
    "With this loss function, CatBoost estimates the mean and variance of the normal distribution optimizing the negative log-likelihood.<br>\n",
    "For each example, CatBoost model returns two values: estimated mean and estimated variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train L2 regressors with uncertainty for the current signal using Ronny & Motti's features.\n",
    "train_sig_df, features = create_signal()\n",
    "test_sig_df, features = create_signal()\n",
    "\n",
    "train_df = train_sig_df[features['ronny'] + ['true_signal']].dropna()\n",
    "X_train_df = train_df[features['ronny']]\n",
    "# train_df = train_sig_df[features['motti'] + ['current_signal']].dropna()\n",
    "# X_train_df = train_df[features['motti']]\n",
    "y_train = train_df['true_signal']\n",
    "\n",
    "test_df = train_sig_df[features['ronny'] + ['true_signal']].dropna()\n",
    "X_test_df = train_df[features['ronny']]\n",
    "# test_df = test_sig_df[features['motti'] + ['current_signal']].dropna()\n",
    "# X_test_df = test_df[features['motti']]\n",
    "\n",
    "y_test = test_df['true_signal']\n",
    "\n",
    "# A pool is just a convenient CatBoost \"container\" class for features and labels.\n",
    "train_pool = Pool(X_train_df, y_train)\n",
    "test_pool = Pool(X_test_df, y_test)\n",
    "\n",
    "uncert_regressor = CatBoostRegressor(loss_function='RMSEWithUncertainty')\n",
    "uncert_regressor.fit(train_pool, silent=True)\n",
    "\n",
    "# Notice that for the 'RMSEWithUncertainty' loss, preds now contains two columns: one for mean & one for variance.\n",
    "preds = uncert_regressor.predict(test_pool)\n",
    "ic(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 8]\n",
    "fig, axes = plt.subplots()\n",
    "x_axis = y_test.index\n",
    "\n",
    "# Reminder: the first columns of preds is the *estimated* mean.\n",
    "plt.plot(x_axis, preds[:,0], color='brown', lw=0.5)\n",
    "# While the second column is the *estimated* variance.\n",
    "plt.fill_between(x_axis, preds[:,0] - 3*np.sqrt(preds[:,1]), preds[:,0] + 3*np.sqrt(preds[:,1]), color='green', alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <img src=\"Electronic_Brain.png\" width=\"100\" style=\"float:left\">\n",
    "Q: What is the correlation coefficient between the predicted variance and the actual variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
